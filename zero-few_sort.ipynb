{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"secrets.json\") as f:\n",
    "    secrets = json.load(f)   \n",
    "Groq_Token = secrets[\"groq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walking_downstairs\n"
     ]
    }
   ],
   "source": [
    "acc1=pd.read_csv(\"./../../Combined/Train/LAYING/Subject_1.csv\")\n",
    "# System Prompts \n",
    "\n",
    "def zsl(acc1):\n",
    "    query = f\"\"\"\n",
    "    * You are a Human activity recognation model. \n",
    "    * Your task is to analyze the sentiment expressed in the given text and classify it as 'Laying', 'Sitting', 'Standing', \"Walking', Walking_Downstairs or Walking_upstairs. \n",
    "    * Provide the Human activity through the acceleration in x, y and z direction.\n",
    "    * Tell only the answer fast. No need for explanation.\n",
    "\n",
    "    Acceleration: {acc1}\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "\n",
    "    return answer.content\n",
    "\n",
    "print(zsl(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walking_downstream\n"
     ]
    }
   ],
   "source": [
    "def fsl(acc):\n",
    "    acc1=pd.read_csv(\"./../../Combined/Train/LAYING/Subject_1.csv\")\n",
    "    acc2=pd.read_csv(\"./../../Combined/Train/SITTING/Subject_1.csv\")\n",
    "    acc3=pd.read_csv(\"./../../Combined/Train/STANDING/Subject_1.csv\")\n",
    "    acc4=pd.read_csv(\"./../../Combined/Train/WALKING/Subject_1.csv\")\n",
    "    acc5=pd.read_csv(\"./../../Combined/Train/WALKING_DOWNSTAIRS/Subject_1.csv\")\n",
    "    acc6=pd.read_csv(\"./../../Combined/Train/WALKING_UPSTAIRS/Subject_1.csv\")\n",
    "    # System Prompts \n",
    "    query = f\"\"\"\n",
    "    * You are a Human activity recognation model. \n",
    "    * Your task is to analyze the sentiment expressed in the given text and classify it as 'Laying', 'Sitting', 'Standing', \"Walking', Walking_Downstairs or Walking_upstairs. \n",
    "    * Provide the Human activity through the acceleration in x, y and z direction use the example to answer question at last.\n",
    "    * Tell only the answer. No need for explanation.\n",
    "    1. Acceleration: {acc1}\n",
    "    Output = Laying\n",
    "    2. Acceleration: {acc2}\n",
    "    Output = Sitting\n",
    "    3. Acceleration: {acc3}\n",
    "    Output = Standing\n",
    "    4. Acceleration: {acc4}\n",
    "    Output = Walking\n",
    "    5. Acceleration: {acc5}\n",
    "    Output = Walking_downstream\n",
    "    6. Acceleration: {acc6}\n",
    "    Output = Walking_upstream\n",
    "\n",
    "    Acceleration: {acc}\n",
    "    Output = ?\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "\n",
    "    return answer.content\n",
    "acc= \"../../Combined/Test/WALKING_DOWNSTAIRS/Subject_10.csv\"\n",
    "print(fsl(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# folder = \"../../Combined/Test/\"\n",
    "# folders = os.listdir(folder)\n",
    "# # print(all_files)\n",
    "# csv_files=[]\n",
    "# for fol in folders:\n",
    "#     print(fol)\n",
    "#     all_files = os.listdir(fol)\n",
    "#     csv_files += [file for file in all_files if file.endswith('.csv')]\n",
    "#     for file_name in csv_files:\n",
    "#         # Construct full file path\n",
    "#         file_path = os.path.join(fol, file_name)\n",
    "        \n",
    "#         # Load the CSV file into a pandas DataFrame\n",
    "#         data = pd.read_csv(file_path)\n",
    "\n",
    "# # for file in folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zsl_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: LAYING\n",
      "Processing folder: SITTING\n",
      "Processing folder: STANDING\n",
      "Processing folder: WALKING\n",
      "Processing folder: WALKING_DOWNSTAIRS\n",
      "Processing folder: WALKING_UPSTAIRS\n",
      "Zero-shot accuracy: 20.37%\n",
      "Few-shot accuracy: 44.44%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming the code you provided is already defined:\n",
    "# - zsl function\n",
    "# - fsl function\n",
    "\n",
    "# Folder containing the test data\n",
    "folder = \"../../Combined/Test/\"\n",
    "folders = os.listdir(folder)\n",
    "\n",
    "# Lists to store actual labels and model predictions\n",
    "actual_labels = []\n",
    "zsl_predictions = []\n",
    "fsl_predictions = []\n",
    "\n",
    "for fol in folders:\n",
    "    print(f\"Processing folder: {fol}\")\n",
    "    all_files = os.listdir(os.path.join(folder, fol))\n",
    "    csv_files = [file for file in all_files if file.endswith('.csv')]\n",
    "    \n",
    "    for file_name in csv_files:\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(folder, fol, file_name)\n",
    "        \n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Assume the actual label is the folder name (e.g., 'SITTING', 'LAYING', etc.)\n",
    "        actual_label = fol\n",
    "        \n",
    "        # Get predictions from zero-shot and few-shot models\n",
    "        zsl_prediction = zsl(data)\n",
    "        fsl_prediction = fsl(data)\n",
    "        \n",
    "        # Append the actual and predicted labels to the lists\n",
    "        actual_labels.append(actual_label)\n",
    "        zsl_predictions.append(zsl_prediction.strip().upper())\n",
    "        fsl_predictions.append(fsl_prediction.strip().upper())\n",
    "\n",
    "# Calculate the accuracy for zero-shot and few-shot predictions\n",
    "zsl_accuracy = accuracy_score(actual_labels, zsl_predictions)\n",
    "fsl_accuracy = accuracy_score(actual_labels, fsl_predictions)\n",
    "\n",
    "# Output the accuracies\n",
    "print(f\"Zero-shot accuracy: {zsl_accuracy * 100:.2f}%\")\n",
    "print(f\"Few-shot accuracy: {fsl_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SITTING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'SITTING',\n",
       " 'WALKING_UPSTAIRS',\n",
       " 'WALKING',\n",
       " 'WALKING',\n",
       " 'SITTING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'SITTING',\n",
       " 'LAYING',\n",
       " 'SITTING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'SITTING',\n",
       " 'STANDING',\n",
       " 'SITTING',\n",
       " 'LAYING',\n",
       " 'STANDING',\n",
       " 'SITTING',\n",
       " 'SITTING',\n",
       " 'SITTING',\n",
       " 'SITTING',\n",
       " 'LAYING',\n",
       " 'SITTING',\n",
       " 'SITTING',\n",
       " 'STANDING',\n",
       " 'LAYING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'SITTING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_UPSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'SITTING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING',\n",
       " 'SITTING',\n",
       " 'SITTING',\n",
       " 'SITTING',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING',\n",
       " 'WALKING_UPSTAIRS',\n",
       " 'WALKING_DOWNSTAIRS',\n",
       " 'WALKING',\n",
       " 'WALKING_DOWNSTAIRS']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zsl_predictions\n",
    "# fsl_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is better Zero shot learning or Few shot learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that few-shot learning have more Accuracy than the zero shot learning because in few shot lerning we give some example data which help the model to identify the human activity more corectly where as zero shot may be less trained and biased all the model may be biased. Few shot learning is better than the zero shot learning in given senerio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the limitation of Zero-shot and Few-shot learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero shot learning is heavily depend on the description provided. if relationship between accelerometer data and the activities are not well define then model give inaccurate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
